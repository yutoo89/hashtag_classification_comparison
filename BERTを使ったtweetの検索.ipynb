{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b95f05c",
   "metadata": {},
   "source": [
    "# tweet類似度検索\n",
    "\n",
    "自然言語処理（NLP）技術であるBERT（Bidirectional Encoder Representations from Transformers）を用いて、テキストメッセージ同士の類似度を計算し、データベースから最も似たテキストメッセージを検索する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1f355",
   "metadata": {},
   "source": [
    "## tweetを格納するテーブルの作成\n",
    "\n",
    "MySQLに接続し、必要なテーブルを作成する。\n",
    "\n",
    "**tweets**\n",
    "\n",
    "| Column name | Data type | Description |\n",
    "|-------------|-----------|-------------|\n",
    "| id          | INT       | テーブルの主キー。自動的にインクリメントされる。 |\n",
    "| text        | VARCHAR(255) | ツイートの本文。 |\n",
    "| created_at  | TIMESTAMP | レコードが作成された日時。デフォルトは現在のタイムスタンプ。 |\n",
    "\n",
    "**tweet_vectors**\n",
    "\n",
    "| Column name | Data type | Description |\n",
    "|-------------|-----------|-------------|\n",
    "| id          | INT       | テーブルの主キー。自動的にインクリメントされる。 |\n",
    "| tweet_id    | INT       | tweetsテーブルのidを参照する外部キー。 |\n",
    "| vector      | JSON      | ツイートのBERTによる数値ベクトル表現。 |\n",
    "| created_at  | TIMESTAMP | レコードが作成された日時。デフォルトは現在のタイムスタンプ。 |\n",
    "\n",
    "`tweets`にはテキストメッセージを格納する。\n",
    "\n",
    "`tweet_vectors`には、`tweets`から取得したテキストメッセージをベクトル表現に変換したものを格納する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb97f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# MySQLデータベースへの接続情報を設定\n",
    "host = \"db\"  # Docker Composeで定義したMySQLサービスのサービス名\n",
    "port = 3306  # MySQLのデフォルトのポート番号\n",
    "user = \"user\"  # MySQLのユーザ名\n",
    "password = \"password\"  # MySQLのパスワード\n",
    "database = \"tweets_db\"  # MySQLのデータベース名\n",
    "\n",
    "def connect_to_database():\n",
    "    # MySQLデータベースに接続\n",
    "    return pymysql.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database=database\n",
    "    )\n",
    "\n",
    "def create_table_if_not_exists(cursor, create_table_query):\n",
    "    # テーブルを作成するSQL文\n",
    "    cursor.execute(create_table_query)\n",
    "\n",
    "    # 変更をコミット（確定）\n",
    "    connection.commit()\n",
    "\n",
    "# tweetsテーブルの作成\n",
    "create_tweets_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tweets (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    text VARCHAR(255),\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# tweet_vectorsテーブルの作成\n",
    "create_tweet_vectors_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tweet_vectors (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    tweet_id INT,\n",
    "    vector JSON,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    FOREIGN KEY (tweet_id) REFERENCES tweets(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# データベースへの接続\n",
    "connection = connect_to_database()\n",
    "\n",
    "# カーソルオブジェクトを作成\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# テーブルの作成\n",
    "create_table_if_not_exists(cursor, create_tweets_table_query)\n",
    "create_table_if_not_exists(cursor, create_tweet_vectors_table_query)\n",
    "\n",
    "# MySQLデータベースとの接続を閉じる\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd019f",
   "metadata": {},
   "source": [
    "## ダミーのtweetの作成と保存\n",
    "\n",
    "ダミーのtweetを作成してMySQLに保存する。\n",
    "\n",
    "具体的には、事前に作成したテキストメッセージ`tweets.csv`を読み込み、`tweets`テーブルに保存する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01653e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, Table, MetaData\n",
    "from sqlalchemy.sql import insert\n",
    "\n",
    "# 設定を読み込む\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# MySQLの設定\n",
    "username = config['DATABASE']['USERNAME']\n",
    "password = config['DATABASE']['PASSWORD']\n",
    "hostname = config['DATABASE']['HOSTNAME']\n",
    "database = config['DATABASE']['DATABASE']\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{username}:{password}@{hostname}/{database}\")\n",
    "\n",
    "# ダミーのツイート\n",
    "df_tweet = pd.read_csv(\"tweets.csv\")\n",
    "\n",
    "# ツイートをデータベースに保存\n",
    "with engine.begin() as connection:\n",
    "    metadata = MetaData()\n",
    "    metadata.bind = engine\n",
    "\n",
    "    tweet_table = Table('tweets', metadata, autoload_with=engine)\n",
    "    for tweet in df_tweet['text']:\n",
    "        stmt = insert(tweet_table).values(text=tweet)\n",
    "        connection.execute(stmt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929e11c",
   "metadata": {},
   "source": [
    "## BERTによるtweetのベクトル変換\n",
    "\n",
    "MySQLに保存されているtweetをBERTを利用してベクトル表現に変換し、`tweet_vectors`に保存する。\n",
    "\n",
    "1. Hugging Faceのtransformersというライブラリを使って、BERTのモデル`bert-base-uncased`とトークナイザをロード\n",
    "2. `tweets`テーブルからテキストメッセージを取得する\n",
    "3. テキストメッセージのベクトル変換\n",
    "  - トークナイザでテキストメッセージをトークン化（モデルに入力できる形式に変換）\n",
    "  - トークンをBERTモデルに入力してベクトル表現を取得する\n",
    "4. ベクトル表現の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131de5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88d81225c5344c3bfc26a873f9a7aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703ed7efcc9c4cd49c5f832ee09141c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606aa82e82b44d47808bad78b0ef6210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbffd232f0c4d90b44050f372a96beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import torch\n",
    "from sqlalchemy import select\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sqlalchemy import create_engine, Table, MetaData\n",
    "from sqlalchemy.sql import insert\n",
    "\n",
    "# 設定を読み込む\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# MySQLの設定\n",
    "username = config['DATABASE']['USERNAME']\n",
    "password = config['DATABASE']['PASSWORD']\n",
    "hostname = config['DATABASE']['HOSTNAME']\n",
    "database = config['DATABASE']['DATABASE']\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{username}:{password}@{hostname}/{database}\")\n",
    "\n",
    "# BERTの設定\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# ツイートをデータベースから取得\n",
    "with engine.begin() as connection:\n",
    "    metadata = MetaData()\n",
    "    metadata.bind = engine\n",
    "    tweet_table = Table('tweets', metadata, autoload_with=engine)\n",
    "    s = select(tweet_table.c.id, tweet_table.c.text)\n",
    "    result = connection.execute(s).fetchall()\n",
    "\n",
    "    for row in result:\n",
    "        tweet_id, text = row\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        vector = outputs.last_hidden_state[:, 0, :].numpy().tolist()\n",
    "        # ベクトルをデータベースに保存\n",
    "        vector_table = Table(\"tweet_vectors\", metadata, autoload_with=engine)\n",
    "        stmt = insert(vector_table).values(tweet_id=tweet_id, vector=vector)\n",
    "        connection.execute(stmt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b9883",
   "metadata": {},
   "source": [
    "## 任意のtweetと類似するtweetの検索\n",
    "\n",
    "与えられた入力テキストに最も類似したtweetをデータベースから検索する。\n",
    "\n",
    "具体的には以下の手順で処理を行う。\n",
    "\n",
    "1. `tweet_vectors`からすべてのベクトル表現を取得\n",
    "2. 検証用のテキストメッセージ`verification_tweets.csv`を読み込み、それぞれに次の処理を行う\n",
    "  - 入力テキストをベクトルに変換\n",
    "  - 入力テキストのベクトルとデータベース内の各ツイートのベクトルのコサイン類似度を計算\n",
    "  - 最も類似度が高いツイートを検索し、そのテキストを`tweets`から取得\n",
    "3. 入力テキストと最も類似すると計算されたテキストを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc4e6b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: 一緒に働く田中が冗談を言って、場の雰囲気が良くなった。\n",
      "Most similar tweet: 犬の喜びそうな顔を見ると、一日の疲れが吹っ飛ぶ。\n",
      "Input text: 久しぶりに砂浜に行ってリフレッシュできた。\n",
      "Most similar tweet: 新しいプログラミング言語を学び始めた。チャレンジは成長につながる。\n",
      "Input text: 昨日読んだ推理小説の最後に驚くべきどんでん返しがあった。\n",
      "Most similar tweet: 昨日観た映画が素晴らしかった。深いメッセージが心に残った。\n",
      "Input text: 不具合の修正ばかりで1日が終わったが、やりきると清々しい。\n",
      "Most similar tweet: 猫の毛づくろいを見ていると癒される。彼らの日常が特別。\n",
      "Input text: サンスベリアを部屋に置くと有害物質を除去してくれるらしい。\n",
      "Most similar tweet: パズルゲームの新レベルをクリア。脳を活性化させる感じが好き。\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import pandas as pd\n",
    "from sqlalchemy import select\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sqlalchemy import create_engine, Table, MetaData\n",
    "\n",
    "# 設定を読み込む\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# MySQLの設定\n",
    "username = config['DATABASE']['USERNAME']\n",
    "password = config['DATABASE']['PASSWORD']\n",
    "hostname = config['DATABASE']['HOSTNAME']\n",
    "database = config['DATABASE']['DATABASE']\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{username}:{password}@{hostname}/{database}\")\n",
    "\n",
    "# BERTの設定\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# データベースからベクトルを取得\n",
    "metadata = MetaData()\n",
    "metadata.bind = engine\n",
    "vector_table = Table(\"tweet_vectors\", metadata, autoload_with=engine)\n",
    "s = select(vector_table.c.tweet_id, vector_table.c.vector)\n",
    "with engine.begin() as connection:\n",
    "    result = connection.execute(s).fetchall()\n",
    "\n",
    "# 'verification_tweets.csv'を読み込む\n",
    "df = pd.read_csv('verification_tweets.csv')\n",
    "\n",
    "# DataFrameのtext列を1件ずつ取り出す\n",
    "for input_text in df['text']:\n",
    "    # 入力テキストをベクトルに変換\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    input_vector = outputs.last_hidden_state[:, 0, :].detach().numpy().tolist()\n",
    "\n",
    "    max_similarity = 0\n",
    "    most_similar_tweet = None\n",
    "\n",
    "    # 各ベクトルと入力テキストのベクトルとの間で類似度を計算\n",
    "    for row in result:\n",
    "        tweet_id, vector = row\n",
    "        similarity = cosine_similarity(input_vector, vector)\n",
    "\n",
    "        # 最も類似度が高いツイートを見つける\n",
    "        if similarity[0][0] > max_similarity:\n",
    "            max_similarity = similarity[0][0]\n",
    "            most_similar_tweet = tweet_id\n",
    "\n",
    "    # 最も類似度が高いツイートの本文を取得\n",
    "    tweet_table = Table(\"tweets\", metadata, autoload_with=engine)\n",
    "    s = select(tweet_table.c.text).where(tweet_table.c.id == most_similar_tweet)\n",
    "    with engine.begin() as connection:\n",
    "        most_similar_tweet_text = connection.execute(s).scalar_one()\n",
    "\n",
    "    # 最も類似度が高いツイートと入力に使ったテキストを表示\n",
    "    print(f\"Input text: {input_text}\")\n",
    "    print(f\"Most similar tweet: {most_similar_tweet_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
